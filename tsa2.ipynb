{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de1807c1-fa99-485c-ac27-519ae5846f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, get_scheduler\n",
    "from torch.optim import AdamW  # <--- Import AdamW from torch.optim\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c984f4e-564c-4f58-bf25-f528b3014808",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('smile-annotations-final.csv', names=['id', 'text', 'category'])\n",
    "df.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1bd4c8d-bd57-4111-976a-2a7bf5ea3286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\|'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\|'\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_30524\\2213153600.py:1: SyntaxWarning: invalid escape sequence '\\|'\n",
      "  df = df[~df.category.str.contains('\\|')]\n"
     ]
    }
   ],
   "source": [
    "df = df[~df.category.str.contains('\\|')]\n",
    "df = df[df.category != 'nocode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "651addfe-f5b2-45db-80dc-cb74e2be2c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_30524\\3070951662.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['label'] = df.category.replace(label_dict)\n"
     ]
    }
   ],
   "source": [
    "label_dict = {label: i for i, label in enumerate(df.category.unique())}\n",
    "df['label'] = df.category.replace(label_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46c220b0-9dc9-413d-aa5d-915983c59570",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, val_idx = train_test_split(df.index, test_size=0.15, stratify=df.label, random_state=17)\n",
    "df.loc[train_idx, 'data_type'] = 'train'\n",
    "df.loc[val_idx, 'data_type'] = 'val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ff56bcd-a177-4b34-acfd-052f07a65c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "264f3392-106d-42c1-a771-564b1845c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(texts):\n",
    "    return tokenizer.batch_encode_plus(\n",
    "        texts,\n",
    "        add_special_tokens=True,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors='pt'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17e36e67-11bd-4221-99f6-5c9f5b0f57b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = df[df.data_type == 'train'].text.values\n",
    "val_texts = df[df.data_type == 'val'].text.values\n",
    "\n",
    "train_encodings = encode_data(train_texts)\n",
    "val_encodings = encode_data(val_texts)\n",
    "\n",
    "train_labels = torch.tensor(df[df.data_type == 'train'].label.values)\n",
    "val_labels = torch.tensor(df[df.data_type == 'val'].label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0a49d50-6ce4-413b-a9fe-4d5d1f426344",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
    "val_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], val_labels)\n",
    "\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c521c21-9813-46c1-b880-bee9dcb3bcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
    "val_loader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5851011-05df-420d-812b-3b9f1735779d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_dict))\n",
    "model = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e301cc9-89f2-4feb-9d9f-76f78be58fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "epochs = 3\n",
    "num_training_steps = epochs * len(train_loader)\n",
    "\n",
    "scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c427d05-208b-430d-bbd1-3b72b832a269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7de4ca77-83b0-4e58-b025-42a6cb5697a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea593fed-57f6-41a5-93a3-c594ee36806e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ca85c23f9f4d3c9dbd60ae763d06e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/158 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.7713\n",
      "Validation F1 Score: 0.7876\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b376d54204a3421fb77ea15b9a8ce409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/158 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.4035\n",
      "Validation F1 Score: 0.8452\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60381745a7fa478498377f45e2923224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/158 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.2574\n",
      "Validation F1 Score: 0.8545\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in loop:\n",
    "        batch = [item.to(device) for item in batch]\n",
    "        input_ids, attention_mask, labels = batch\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        loop.set_description(f\"Loss {loss.item():.4f}\")\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_labels_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = [item.to(device) for item in batch]\n",
    "            input_ids, attention_mask, labels = batch\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            val_preds.append(logits.detach().cpu().numpy())\n",
    "            val_labels_list.append(labels.detach().cpu().numpy())\n",
    "\n",
    "    val_preds = np.concatenate(val_preds, axis=0)\n",
    "    val_labels = np.concatenate(val_labels_list, axis=0)\n",
    "\n",
    "    f1 = compute_f1(val_preds, val_labels)\n",
    "    print(f\"Validation F1 Score: {f1:.4f}\")\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8421f2f9-859c-45b8-930a-63624eadd44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to bert_sentiment_model.pt\n",
      "Tokenizer saved to bert_tokenizer/\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model_save_path = \"bert_sentiment_model.pt\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(\"bert_tokenizer/\")\n",
    "print(\"Tokenizer saved to bert_tokenizer/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aab4df0-e57e-4df3-9bbf-7dc92d9f12e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
